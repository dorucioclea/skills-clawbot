#!/usr/bin/env python3
"""
Claw Ethics Checker - Core Module
ä¼¦ç†åˆè§„æ£€æŸ¥å™¨æ ¸å¿ƒæ¨¡å—
"""

import json
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from enum import Enum
import re

class RiskLevel(Enum):
    """é£é™©ç­‰çº§æšä¸¾"""
    LOW = "low"      # ä½é£é™©
    MEDIUM = "medium" # ä¸­é£é™©
    HIGH = "high"    # é«˜é£é™©

@dataclass
class EthicsCheckResult:
    """ä¼¦ç†æ£€æŸ¥ç»“æœ"""
    risk_level: RiskLevel          # é£é™©ç­‰çº§
    is_compliant: bool             # æ˜¯å¦åˆè§„
    needs_human_review: bool       # æ˜¯å¦éœ€è¦äººå·¥å®¡æ ¸
    recommendations: List[str]     # å»ºè®®åˆ—è¡¨
    warnings: List[str]            # è­¦å‘Šåˆ—è¡¨
    legal_issues: List[str]        # æ³•å¾‹é—®é¢˜åˆ—è¡¨
    ethical_concerns: List[str]    # ä¼¦ç†é—®é¢˜åˆ—è¡¨
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'risk_level': self.risk_level.value,
            'is_compliant': self.is_compliant,
            'needs_human_review': self.needs_human_review,
            'recommendations': self.recommendations,
            'warnings': self.warnings,
            'legal_issues': self.legal_issues,
            'ethical_concerns': self.ethical_concerns
        }
    
    def to_json(self) -> str:
        """è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²"""
        return json.dumps(self.to_dict(), ensure_ascii=False, indent=2)

class EthicsChecker:
    """ä¼¦ç†åˆè§„æ£€æŸ¥å™¨"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–ä¼¦ç†æ£€æŸ¥å™¨
        
        Args:
            config: é…ç½®å­—å…¸ï¼Œå¯åŒ…å«ï¼š
                - risk_threshold: é£é™©é˜ˆå€¼ (low/medium/high)
                - require_human_review: æ˜¯å¦è¦æ±‚äººå·¥å®¡æ ¸
                - log_decisions: æ˜¯å¦è®°å½•å†³ç­–
        """
        self.config = config or {}
        self.risk_threshold = self.config.get('risk_threshold', 'medium')
        self.require_human_review = self.config.get('require_human_review', True)
        self.log_decisions = self.config.get('log_decisions', True)
        
        # åŠ è½½è§„åˆ™æ•°æ®åº“
        self.load_rules()
        
        # å†³ç­–æ—¥å¿—
        self.decision_log: List[Dict[str, Any]] = []
    
    def load_rules(self):
        """åŠ è½½ä¼¦ç†è§„åˆ™æ•°æ®åº“"""
        # è¿™é‡Œå¯ä»¥è¿æ¥å¤–éƒ¨æ•°æ®åº“æˆ–åŠ è½½æœ¬åœ°è§„åˆ™æ–‡ä»¶
        # ç›®å‰ä½¿ç”¨å†…ç½®è§„åˆ™
        self.rules = {
            'privacy': {
                'keywords': ['personal data', 'private information', 'user data', 'æ•æ„Ÿä¿¡æ¯', 'ä¸ªäººæ•°æ®', 'éšç§'],
                'rules': [
                    'ä¸æ”¶é›†ä¸ªäººèº«ä»½ä¿¡æ¯',
                    'ä¸å­˜å‚¨æ•æ„Ÿæ•°æ®', 
                    'éµå®ˆGDPR/CCPAç­‰éšç§æ³•è§„',
                    'æ•°æ®åŒ¿ååŒ–å¤„ç†',
                    'è·å–ç”¨æˆ·æ˜ç¡®åŒæ„'
                ]
            },
            'security': {
                'keywords': ['hack', 'bypass', 'exploit', 'attack', 'å…¥ä¾µ', 'ç»•è¿‡', 'ç ´è§£', 'æ¼æ´'],
                'rules': [
                    'ä¸è¿›è¡Œæœªæˆæƒè®¿é—®',
                    'ä¸ç»•è¿‡å®‰å…¨æªæ–½',
                    'ä¸è¿›è¡ŒDDoSæ”»å‡»',
                    'ä¸åˆ©ç”¨å®‰å…¨æ¼æ´',
                    'éµå®ˆè®¡ç®—æœºå®‰å…¨æ³•'
                ]
            },
            'legal': {
                'keywords': ['competitor', 'scrape', 'monitor', 'ç«äº‰å¯¹æ‰‹', 'çˆ¬å–', 'ç›‘æ§'],
                'rules': [
                    'éµå®ˆå½“åœ°æ³•å¾‹æ³•è§„',
                    'å°Šé‡çŸ¥è¯†äº§æƒ',
                    'ä¸å‚ä¸éæ³•æ´»åŠ¨',
                    'éµå®ˆåä¸æ­£å½“ç«äº‰æ³•',
                    'éµå®ˆæ•°æ®ä¿æŠ¤æ³•'
                ]
            },
            'ethical': {
                'keywords': ['harm', 'damage', 'deceive', 'æ¬ºéª—', 'æŸå®³', 'ä¼¤å®³'],
                'rules': [
                    'ä¸æŸå®³ä»–äººåˆ©ç›Š',
                    'ä¿æŒé€æ˜åº¦',
                    'å…¬å¹³ç«äº‰',
                    'ä¸æ¬ºéª—ç”¨æˆ·',
                    'æ‰¿æ‹…ç¤¾ä¼šè´£ä»»'
                ]
            }
        }
    
    def analyze_task(self, task_description: str, task_details: Dict[str, Any]) -> EthicsCheckResult:
        """
        åˆ†æä»»åŠ¡ä¼¦ç†åˆè§„æ€§
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            task_details: ä»»åŠ¡è¯¦ç»†ä¿¡æ¯
            
        Returns:
            EthicsCheckResult: æ£€æŸ¥ç»“æœ
        """
        # åˆå§‹åŒ–ç»“æœ
        recommendations: List[str] = []
        warnings: List[str] = []
        legal_issues: List[str] = []
        ethical_concerns: List[str] = []
        
        # åˆå¹¶æ–‡æœ¬ç”¨äºæ£€æŸ¥
        combined_text = f"{task_description} {json.dumps(task_details, ensure_ascii=False)}".lower()
        
        # æ£€æŸ¥å„ä¸ªç»´åº¦
        privacy_violation = self._check_privacy_violation(combined_text)
        security_violation = self._check_security_violation(combined_text)
        legal_checks = self._check_legal_compliance(combined_text, task_details)
        ethical_checks = self._check_ethical_concerns(combined_text)
        
        # æ”¶é›†é—®é¢˜
        if privacy_violation:
            warnings.append('å¯èƒ½æ¶‰åŠéšç§ä¾µçŠ¯')
            ethical_concerns.append('éšç§ä¿æŠ¤é—®é¢˜')
            recommendations.append('è¿›è¡Œæ•°æ®åŒ¿ååŒ–å¤„ç†')
            recommendations.append('è·å–ç”¨æˆ·æ˜ç¡®åŒæ„')
        
        if security_violation:
            warnings.append('å¯èƒ½æ¶‰åŠå®‰å…¨è¿è§„')
            legal_issues.append('å¯èƒ½è¿åè®¡ç®—æœºå®‰å…¨æ³•')
            recommendations.append('ä½¿ç”¨åˆæ³•æˆæƒçš„æ–¹æ³•')
            recommendations.append('é¿å…å®‰å…¨ç»•è¿‡è¡Œä¸º')
        
        legal_issues.extend(legal_checks)
        ethical_concerns.extend(ethical_checks)
        
        # æ ¹æ®é—®é¢˜æ•°é‡ç”Ÿæˆé¢å¤–å»ºè®®
        if legal_checks:
            recommendations.append('å’¨è¯¢æ³•å¾‹ä¸“å®¶')
            recommendations.append('å®¡æŸ¥ç›¸å…³æ³•å¾‹æ³•è§„')
        
        if ethical_checks:
            recommendations.append('è¿›è¡Œä¼¦ç†å½±å“è¯„ä¼°')
            recommendations.append('è€ƒè™‘æ›¿ä»£æ–¹æ¡ˆ')
        
        # ç¡®å®šé£é™©ç­‰çº§
        risk_level = self._determine_risk_level(
            len(warnings), 
            len(legal_issues), 
            len(ethical_concerns)
        )
        
        # ç”Ÿæˆæ€»ä½“å»ºè®®
        if risk_level == RiskLevel.HIGH:
            recommendations.insert(0, 'å»ºè®®æ‹’ç»æ­¤ä»»åŠ¡')
            recommendations.insert(1, 'å¿…é¡»è¿›è¡Œäººå·¥å®¡æ ¸')
        elif risk_level == RiskLevel.MEDIUM:
            recommendations.insert(0, 'å»ºè®®ä¿®æ”¹ä»»åŠ¡æ–¹æ¡ˆ')
            recommendations.insert(1, 'å»ºè®®è¿›è¡Œäººå·¥å®¡æ ¸')
        else:
            recommendations.insert(0, 'ä»»åŠ¡åŸºæœ¬åˆè§„')
            if self.require_human_review:
                recommendations.insert(1, 'å»ºè®®ç®€å•å®¡æ ¸')
        
        # æ˜¯å¦éœ€è¦äººå·¥å®¡æ ¸
        needs_human_review = (
            risk_level in [RiskLevel.MEDIUM, RiskLevel.HIGH] or 
            self.require_human_review
        )
        
        # åˆ›å»ºç»“æœ
        result = EthicsCheckResult(
            risk_level=risk_level,
            is_compliant=(risk_level == RiskLevel.LOW),
            needs_human_review=needs_human_review,
            recommendations=list(set(recommendations)),  # å»é‡
            warnings=list(set(warnings)),
            legal_issues=list(set(legal_issues)),
            ethical_concerns=list(set(ethical_concerns))
        )
        
        # è®°å½•å†³ç­–
        if self.log_decisions:
            self._log_decision(task_description, task_details, result)
        
        return result
    
    def _check_privacy_violation(self, text: str) -> bool:
        """æ£€æŸ¥éšç§ä¾µçŠ¯"""
        privacy_keywords = self.rules['privacy']['keywords']
        for keyword in privacy_keywords:
            if keyword.lower() in text:
                return True
        return False
    
    def _check_security_violation(self, text: str) -> bool:
        """æ£€æŸ¥å®‰å…¨è¿è§„"""
        security_keywords = self.rules['security']['keywords']
        for keyword in security_keywords:
            if keyword.lower() in text:
                return True
        return False
    
    def _check_legal_compliance(self, text: str, task_details: Dict[str, Any]) -> List[str]:
        """æ£€æŸ¥æ³•å¾‹åˆè§„æ€§"""
        issues = []
        
        # æ£€æŸ¥ç«äº‰å¯¹æ‰‹ç›¸å…³
        if any(word in text for word in ['competitor', 'ç«äº‰å¯¹æ‰‹']):
            if 'scrape' in text or 'çˆ¬å–' in text:
                issues.append('å¯èƒ½è¿ååä¸æ­£å½“ç«äº‰æ³•')
                issues.append('éœ€è¦å®¡æŸ¥æ•°æ®è·å–æ–¹å¼')
        
        # æ£€æŸ¥ç›‘æ§ç›¸å…³
        if 'monitor' in text or 'ç›‘æ§' in text:
            if 'user' in text or 'ç”¨æˆ·' in text:
                issues.append('éœ€è¦ç”¨æˆ·æ˜ç¡®åŒæ„')
                issues.append('å¯èƒ½æ¶‰åŠéšç§æ³•è§„')
        
        # æ£€æŸ¥æ•°æ®æ¥æº
        data_source = task_details.get('data_source', '').lower()
        if 'unauthorized' in data_source or 'æœªæˆæƒ' in data_source:
            issues.append('æ•°æ®æ¥æºæœªæˆæƒ')
            issues.append('å¯èƒ½è¿åæ•°æ®ä¿æŠ¤æ³•')
        
        return issues
    
    def _check_ethical_concerns(self, text: str) -> List[str]:
        """æ£€æŸ¥ä¼¦ç†é—®é¢˜"""
        concerns = []
        
        # æ£€æŸ¥æŸå®³ç›¸å…³
        if any(word in text for word in ['harm', 'damage', 'æŸå®³', 'ä¼¤å®³']):
            concerns.append('å¯èƒ½æŸå®³ä»–äººåˆ©ç›Š')
        
        # æ£€æŸ¥æ¬ºéª—ç›¸å…³
        if any(word in text for word in ['deceive', 'cheat', 'æ¬ºéª—', 'ä½œå¼Š']):
            concerns.append('æ¶‰åŠæ¬ºéª—è¡Œä¸º')
            concerns.append('è¿åè¯šä¿¡åŸåˆ™')
        
        # æ£€æŸ¥å…¬å¹³æ€§
        if 'unfair' in text or 'ä¸å…¬å¹³' in text:
            concerns.append('å¯èƒ½æ¶‰åŠä¸å…¬å¹³ç«äº‰')
        
        return concerns
    
    def _determine_risk_level(self, warnings: int, legal_issues: int, ethical_concerns: int) -> RiskLevel:
        """ç¡®å®šé£é™©ç­‰çº§"""
        total_issues = warnings + legal_issues + ethical_concerns
        
        # é«˜é£é™©æ¡ä»¶
        if legal_issues > 0 or total_issues >= 3:
            return RiskLevel.HIGH
        
        # ä¸­é£é™©æ¡ä»¶
        elif total_issues >= 1:
            return RiskLevel.MEDIUM
        
        # ä½é£é™©
        else:
            return RiskLevel.LOW
    
    def _log_decision(self, task_description: str, task_details: Dict[str, Any], result: EthicsCheckResult):
        """è®°å½•å†³ç­–"""
        log_entry = {
            'task_description': task_description,
            'task_details': task_details,
            'result': result.to_dict(),
            'timestamp': self._get_timestamp()
        }
        self.decision_log.append(log_entry)
    
    def _get_timestamp(self) -> str:
        """è·å–æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().isoformat()
    
    def get_decision_log(self) -> List[Dict[str, Any]]:
        """è·å–å†³ç­–æ—¥å¿—"""
        return self.decision_log
    
    def clear_decision_log(self):
        """æ¸…ç©ºå†³ç­–æ—¥å¿—"""
        self.decision_log.clear()
    
    def export_decision_log(self, filepath: str):
        """å¯¼å‡ºå†³ç­–æ—¥å¿—åˆ°æ–‡ä»¶"""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.decision_log, f, ensure_ascii=False, indent=2)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæ£€æŸ¥å™¨
    checker = EthicsChecker()
    
    # æµ‹è¯•ç”¨ä¾‹
    test_task = {
        'description': 'åˆ†æç”¨æˆ·è¡Œä¸ºæ•°æ®ä»¥æ”¹è¿›äº§å“',
        'client': 'ç§‘æŠ€å…¬å¸',
        'methods': ['data_analysis', 'machine_learning'],
        'data_source': 'ç”¨æˆ·åŒæ„æ”¶é›†çš„æ•°æ®'
    }
    
    result = checker.analyze_task('ç”¨æˆ·è¡Œä¸ºåˆ†æ', test_task)
    
    print("ğŸ§ª Claw Ethics Checker ç¤ºä¾‹")
    print("=" * 50)
    print(f"ä»»åŠ¡: {test_task['description']}")
    print(f"é£é™©ç­‰çº§: {result.risk_level.value}")
    print(f"æ˜¯å¦åˆè§„: {result.is_compliant}")
    print(f"éœ€è¦äººå·¥å®¡æ ¸: {result.needs_human_review}")
    print(f"å»ºè®®: {', '.join(result.recommendations)}")
    
    if result.warnings:
        print(f"è­¦å‘Š: {', '.join(result.warnings)}")
    
    if result.legal_issues:
        print(f"æ³•å¾‹é—®é¢˜: {', '.join(result.legal_issues)}")
    
    if result.ethical_concerns:
        print(f"ä¼¦ç†é—®é¢˜: {', '.join(result.ethical_concerns)}")